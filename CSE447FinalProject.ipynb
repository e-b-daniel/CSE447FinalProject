{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Installs\n",
        "!pip install transformers"
      ],
      "metadata": {
        "id": "_mwdSZIfz2IC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Imports\n",
        "from datasets import load_metric, load_dataset\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "import torch"
      ],
      "metadata": {
        "id": "UOzNehD5u7KZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Simple Parameter-efficient Fine-tuning for Transformer-based Masked Language-models"
      ],
      "metadata": {
        "id": "zCiyhvKaQ6Ji"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jB-L7tYL03T6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Training the model"
      ],
      "metadata": {
        "id": "wTNVOU-JgOYE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Getting the model\n",
        "model =\n",
        "\n",
        "# Get tokenizer"
      ],
      "metadata": {
        "id": "TE4RALDQgEQp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OzVsKJFeQi6y"
      },
      "outputs": [],
      "source": [
        "# Training function\n",
        "\n",
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "def train():\n",
        "  model.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Evaluation"
      ],
      "metadata": {
        "id": "gEvDxCBDgGaN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## GLUE Benchmark"
      ],
      "metadata": {
        "id": "NPi6y4LkuPrV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the GLUE evaluator\n",
        "glue_metric = load_metric(\"glue\")\n",
        "\n",
        "# Example: Model predictions and ground truth labels\n",
        "model_predictions = {...}  # Dictionary containing model predictions for each task\n",
        "ground_truth_labels = {...}  # Dictionary containing ground truth labels for each task\n",
        "\n",
        "# Calculate evaluation metrics for each task\n",
        "task_metrics = {}\n",
        "for task_name, predictions in model_predictions.items():\n",
        "    labels = ground_truth_labels[task_name]\n",
        "    task_metric = glue_metric.compute(predictions=predictions, references=labels)\n",
        "    task_metrics[task_name] = task_metric\n",
        "\n",
        "# Optionally, aggregate performance across all tasks to compute the GLUE score\n",
        "glue_score = glue_metric.compute(metric_key=\"glue\", task_metrics=task_metrics)\n",
        "print(\"GLUE Score:\", glue_score)\n",
        "\n",
        "dataset = load_dataset('glue', )"
      ],
      "metadata": {
        "id": "AwF7SICks3q-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##SQuAD Dataset"
      ],
      "metadata": {
        "id": "_j7RbkL_uyfJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load SQuAD dataset\n",
        "squad = load_dataset(\"squad\")\n",
        "\n",
        "# Load tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "# Tokenize input text\n",
        "tokenized_squad = squad.map(lambda example: tokenizer(example[\"context\"], example[\"question\"], truncation=True, padding=\"max_length\"), batched=True)"
      ],
      "metadata": {
        "id": "NhEit7cKu1Cp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Results"
      ],
      "metadata": {
        "id": "sHSKCjxyRbuK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Plots/Tables"
      ],
      "metadata": {
        "id": "c6NNQQ6MRe7-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This would be a plot of size of training data vs. percentage of match (F1), similar to the graph given in the paper.\n",
        "plt.plot(data=data, x='train_size', y='percentage_match', hue='model')"
      ],
      "metadata": {
        "id": "FDfojpeARd77"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ABLCITe7np-u"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}